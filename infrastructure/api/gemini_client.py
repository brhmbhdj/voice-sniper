"""
Client pour l'API Gemini (Google AI) avec d√©couverte dynamique des mod√®les.
Impl√©mente le port LLMProvider d√©fini dans le domaine.
Utilise le pattern "Dynamic Discovery" pour √©viter les erreurs 404.
"""

import re
from typing import Optional

import google.generativeai as genai

from domain.models import Prospect, Trigger, Script, Language
from domain.ports import LLMProvider
from infrastructure.config import obtenir_configuration


class GeminiClient(LLMProvider):
    """
    Adaptateur pour le mod√®le de langage Gemini de Google.
    Impl√©mente une d√©couverte dynamique des mod√®les pour √©viter les erreurs 404.
    """

    def __init__(
        self,
        cle_api: Optional[str] = None,
        nom_modele: Optional[str] = None
    ):
        """
        Initialise le client Gemini avec configuration minimale.
        Le mod√®le n'est pas configur√© ici (lazy loading).
        
        Args:
            cle_api: Cl√© API Gemini (si None, utilise la config)
            nom_modele: Nom du mod√®le pr√©f√©r√© (si None, utilise la config)
        """
        config = obtenir_configuration()
        self.cle_api = cle_api or config.gemini_cle_api
        self.nom_modele_preference = nom_modele or config.gemini_modele
        
        # Configuration de l'API Gemini (sans mod√®le sp√©cifique)
        genai.configure(api_key=self.cle_api)
        
        # Le mod√®le sera initialis√© √† la demande (lazy loading)
        self._modele: Optional[genai.GenerativeModel] = None
        self._nom_modele_effectif: Optional[str] = None
        
        # Configuration de la g√©n√©ration (ind√©pendante du mod√®le)
        self.config_generation = {
            "temperature": 0.7,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 2048,
        }

    def _get_model(self) -> genai.GenerativeModel:
        """
        Lazy loading du mod√®le Gemini avec d√©couverte dynamique.
        Si le mod√®le n'est pas encore initialis√©, tente de le d√©couvrir automatiquement.
        
        Returns:
            Instance de GenerativeModel pr√™te √† l'emploi
            
        Raises:
            Exception: Si aucun mod√®le ne peut √™tre trouv√©
        """
        # Si d√©j√† initialis√©, retourne le mod√®le en cache
        if self._modele is not None:
            return self._modele
        
        print(f"ü§ñ Initialisation dynamique du mod√®le Gemini...")
        print(f"   Pr√©f√©rence utilisateur : {self.nom_modele_preference}")
        
        # √âtape 1 : Essayer le mod√®le pr√©f√©r√© (s'il est sp√©cifi√©)
        if self.nom_modele_preference:
            try:
                print(f"   Tentative avec : {self.nom_modele_preference}")
                modele = genai.GenerativeModel(self.nom_modele_preference)
                # Test rapide pour v√©rifier que le mod√®le existe
                modele._model_id  # Acc√®de √† l'ID pour valider
                self._modele = modele
                self._nom_modele_effectif = self.nom_modele_preference
                print(f"‚úÖ Mod√®le '{self._nom_modele_effectif}' charg√© avec succ√®s")
                return self._modele
            except Exception as erreur:
                print(f"   ‚ö†Ô∏è  √âchec avec '{self.nom_modele_preference}' : {str(erreur)[:100]}")
        
        # √âtape 2 : Liste des noms √† tester (ordre de pr√©f√©rence)
        candidats_a_tester = [
            "gemini-1.5-flash-latest",
            "gemini-1.5-flash",
            "gemini-1.5-flash-001",
            "gemini-1.5-pro-latest",
            "gemini-1.5-pro",
            "gemini-1.5-pro-001",
            "gemini-1.0-pro-latest",
            "gemini-1.0-pro",
            "gemini-pro",
        ]
        
        for nom_candidat in candidats_a_tester:
            if nom_candidat == self.nom_modele_preference:
                continue  # D√©j√† test√© ci-dessus
            try:
                print(f"   Tentative avec : {nom_candidat}")
                modele = genai.GenerativeModel(nom_candidat)
                modele._model_id  # Validation
                self._modele = modele
                self._nom_modele_effectif = nom_candidat
                print(f"‚úÖ Mod√®le '{self._nom_modele_effectif}' charg√© avec succ√®s")
                return self._modele
            except Exception:
                continue  # Essayer le suivant
        
        # √âtape 3 : D√©couverte dynamique via l'API
        print(f"   üîç D√©couverte dynamique via list_models()...")
        try:
            modele_trouve = self._decouvrir_modele_dynamiquement()
            if modele_trouve:
                self._modele = modele_trouve
                print(f"‚úÖ Mod√®le d√©couvert dynamiquement : '{self._nom_modele_effectif}'")
                return self._modele
        except Exception as erreur:
            print(f"   ‚ùå √âchec de la d√©couverte dynamique : {str(erreur)}")
        
        # √âtape 4 : √âchec complet
        raise Exception(
            "Impossible d'initialiser un mod√®le Gemini. "
            "V√©rifiez votre cl√© API et les permissions du projet."
        )

    def _decouvrir_modele_dynamiquement(self) -> Optional[genai.GenerativeModel]:
        """
        D√©couvre automatiquement un mod√®le disponible via l'API Gemini.
        
        Strat√©gie de s√©lection :
        1. Prendre le premier mod√®le contenant "flash" (rapide et √©conomique)
        2. Sinon prendre le premier contenant "pro" (plus puissant)
        3. Sinon prendre n'importe quel mod√®le Gemini disponible
        
        Returns:
            GenerativeModel initialis√© ou None si √©chec
        """
        try:
            # Liste tous les mod√®les disponibles
            modeles_disponibles = list(genai.list_models())
            
            if not modeles_disponibles:
                raise Exception("Aucun mod√®le trouv√© dans la liste")
            
            # Filtrer uniquement les mod√®les de g√©n√©ration (pas les embeddings)
            modeles_generatifs = [
                m for m in modeles_disponibles 
                if hasattr(m, 'name') and 'embed' not in m.name.lower()
            ]
            
            print(f"   üìã {len(modeles_generatifs)} mod√®le(s) g√©n√©ratif(s) trouv√©(s)")
            
            # Affiche les mod√®les trouv√©s pour debug
            for m in modeles_generatifs[:5]:
                print(f"      - {m.name}")
            
            # Strat√©gie de s√©lection par priorit√©
            modele_choisi = None
            
            # Priorit√© 1 : Flash (rapide, √©conomique)
            for m in modeles_generatifs:
                if 'flash' in m.name.lower():
                    modele_choisi = m.name
                    break
            
            # Priorit√© 2 : Pro (plus puissant)
            if not modele_choisi:
                for m in modeles_generatifs:
                    if 'pro' in m.name.lower():
                        modele_choisi = m.name
                        break
            
            # Priorit√© 3 : N'importe quel mod√®le Gemini
            if not modele_choisi and modeles_generatifs:
                modele_choisi = modeles_generatifs[0].name
            
            if modele_choisi:
                self._nom_modele_effectif = modele_choisi
                print(f"   üéØ Mod√®le s√©lectionn√© : {modele_choisi}")
                return genai.GenerativeModel(modele_choisi)
            
            return None
            
        except Exception as erreur:
            print(f"   Erreur lors de la d√©couverte : {str(erreur)}")
            return None

    def generer_script_cold_call(
        self,
        prospect: Prospect,
        trigger: Trigger,
        langue: Language,
        ton: str = "professionnel"
    ) -> Script:
        """
        G√©n√®re un script de cold call personnalis√© avec Gemini.
        
        Args:
            prospect: Informations sur le prospect
            trigger: √âv√©nement d√©clencheur
            langue: Langue souhait√©e pour le script
            ton: Ton de la conversation
            
        Returns:
            Script complet g√©n√©r√© par l'IA
            
        Raises:
            Exception: Si la g√©n√©ration √©choue
        """
        try:
            # R√©cup√©ration du mod√®le (avec lazy loading et d√©couverte dynamique)
            modele = self._get_model()
            
            # Construction du prompt
            prompt = self._construire_prompt(prospect, trigger, langue, ton)
            
            print(f"üìù G√©n√©ration du script avec {self._nom_modele_effectif}...")
            
            # G√©n√©ration avec Gemini
            reponse = modele.generate_content(
                prompt,
                generation_config=self.config_generation
            )
            
            # Parsing de la r√©ponse
            contenu_genere = reponse.text
            
            return self._parser_script(contenu_genere, langue)
            
        except Exception as erreur:
            raise Exception(
                f"Erreur lors de la g√©n√©ration du script avec Gemini : {str(erreur)}"
            )

    def detecter_langue_ideale(self, prospect: Prospect) -> Language:
        """
        D√©tecte la langue id√©ale pour contacter un prospect.
        Utilise Gemini pour analyser les indices (entreprise, secteur, notes)
        et d√©terminer la meilleure langue pour le cold call.
        
        Args:
            prospect: Informations du prospect
            
        Returns:
            Langue recommand√©e (FRENCH, ENGLISH, etc.)
        """
        try:
            modele = self._get_model()
            
            # Construction du contexte pour l'analyse
            contexte = f"""
            Analyse ce prospect et d√©termine la langue la plus appropri√©e pour un cold call professionnel.
            
            INFORMATIONS DU PROSPECT :
            - Nom : {prospect.nom_complet}
            - Entreprise : {prospect.entreprise}
            - Secteur : {prospect.secteur_activite or "Non sp√©cifi√©"}
            """
            
            # Ajout des notes enrichies si disponibles
            if prospect.notes_enrichies and prospect.notes_enrichies.notes_brutes:
                contexte += f"\n- Notes : {prospect.notes_enrichies.notes_brutes[:500]}"
            
            contexte += """
            
            INSTRUCTIONS :
            R√©ponds UNIQUEMENT avec le code langue ISO √† 2 lettres parmi :
            - "fr" pour fran√ßais
            - "en" pour anglais  
            - "es" pour espagnol
            - "de" pour allemand
            - "it" pour italien
            
            R√®gles de d√©cision :
            - Entreprise fran√ßaise ou nom fran√ßais ‚Üí "fr"
            - Entreprise internationale/tech/saas ‚Üí "en" 
            - Entreprise allemande ‚Üí "de"
            - Indices g√©ographiques dans les notes
            - Langue des notes si r√©dig√©es en fran√ßais/anglais
            
            R√©ponds uniquement avec le code (ex: "fr" ou "en").
            """
            
            reponse = modele.generate_content(
                contexte,
                generation_config={"temperature": 0.1, "max_output_tokens": 10}
            )
            
            # Extraction du code langue
            texte_reponse = reponse.text.strip().lower()
            
            # Mapping des codes vers l'enum
            mapping_langues = {
                "fr": Language.FRENCH,
                "en": Language.ENGLISH,
                "es": Language.SPANISH,
                "de": Language.GERMAN,
                "it": Language.ITALIAN
            }
            
            # Cherche le code dans la r√©ponse
            for code, langue in mapping_langues.items():
                if code in texte_reponse:
                    return langue
            
            # D√©tection par d√©faut bas√©e sur l'entreprise
            return self._detecter_langue_par_defaut(prospect)
            
        except Exception:
            # En cas d'erreur, utiliser la d√©tection par d√©faut
            return self._detecter_langue_par_defaut(prospect)

    def _detecter_langue_par_defaut(self, prospect: Prospect) -> Language:
        """
        D√©tection de langue par d√©faut bas√©e sur des r√®gles simples.
        
        Args:
            prospect: Prospect √† analyser
            
        Returns:
            Langue d√©tect√©e
        """
        entreprise_lower = prospect.entreprise.lower()
        nom_lower = prospect.nom_complet.lower()
        
        # Mots-cl√©s fran√ßais
        mots_fr = ["sas", "france", "paris", "lyon", "marseille", "bordeaux", "lille", 
                   "toulouse", "nantes", "strasbourg", "fr", "fran√ßais"]
        
        # Mots-cl√©s internationaux (anglais probable)
        mots_intl = ["inc", "corp", "llc", "ltd", "gmbh", "ag", "bv", "sl", "global", 
                     "international", "ai", "tech", "labs", "io", "app", "cloud"]
        
        # Mots-cl√©s allemands
        mots_de = ["gmbh", "ag", "kg", "germany", "deutschland", "berlin", "munich"]
        
        # V√©rification
        for mot in mots_fr:
            if mot in entreprise_lower or mot in nom_lower:
                return Language.FRENCH
        
        for mot in mots_de:
            if mot in entreprise_lower or mot in nom_lower:
                return Language.GERMAN
                
        for mot in mots_intl:
            if mot in entreprise_lower or mot in nom_lower:
                return Language.ENGLISH
        
        # Par d√©faut : anglais pour le B2B international
        return Language.ENGLISH

    def _construire_prompt(
        self,
        prospect: Prospect,
        trigger: Trigger,
        langue: Language,
        ton: str
    ) -> str:
        """
        Construit le prompt pour la g√©n√©ration du script.
        Utilise les notes enrichies si disponibles pour ultra-personnalisation.
        
        Args:
            prospect: Informations du prospect
            trigger: √âv√©nement d√©clencheur
            langue: Langue souhait√©e
            ton: Ton du script
            
        Returns:
            Prompt format√© pour Gemini
        """
        # R√©cup√©ration du nom du vendeur depuis la config
        from infrastructure.config import obtenir_configuration
        config = obtenir_configuration()
        nom_vendeur = config.utilisateur_nom
        
        # Mapping des langues pour le prompt
        langues_texte = {
            Language.FRENCH: "fran√ßais",
            Language.ENGLISH: "anglais",
            Language.SPANISH: "espagnol",
            Language.GERMAN: "allemand",
            Language.ITALIAN: "italien"
        }
        
        nom_langue = langues_texte.get(langue, "fran√ßais")
        
        # Toutes les informations du prospect depuis Notion
        titre_info = f"Titre/Poste : {prospect.titre}" if prospect.titre else ""
        taille_info = f"Taille entreprise : {prospect.taille}" if prospect.taille else ""
        statut_info = f"Statut pipeline : {prospect.statut}" if prospect.statut else ""
        secteur_info = f"Secteur : {prospect.secteur_activite}" if prospect.secteur_activite else ""
        
        # Construction du contexte enrichi depuis les notes Notion
        contexte_enrichi = ""
        if prospect.notes_enrichies:
            notes = prospect.notes_enrichies
            
            # LES NOTES BRUTES SONT LE PLUS IMPORTANT
            if notes.notes_brutes:
                contexte_enrichi += f"\nüìù NOTES BRUTES DU PROSPECT (√† utiliser obligatoirement) :\n{notes.notes_brutes}\n"
            
            if notes.situation_actuelle:
                contexte_enrichi += f"\nüìä SITUATION ACTUELLE :\n{notes.situation_actuelle}\n"
            
            if notes.pain_points:
                contexte_enrichi += "\nüéØ PAIN POINTS IDENTIFI√âS :\n"
                for i, pain in enumerate(notes.pain_points, 1):
                    contexte_enrichi += f"{i}. {pain}\n"
            
            if notes.value_proposition:
                contexte_enrichi += f"\nüíé ANGLES DE VALEUR :\n{notes.value_proposition}\n"
        
        # Contexte des notes - UTILISATION OBLIGATOIRE
        notes_context = ""
        if prospect.notes_enrichies and prospect.notes_enrichies.notes_brutes:
            notes = prospect.notes_enrichies.notes_brutes[:3000]  # Limite pour √©viter de d√©passer les tokens
            notes_context = f"""
üö® CONTEXTE OBLIGATOIRE - NOTES NOTION DU PROSPECT :
{notes}
üö® FIN DES NOTES - TU DOIS UTILISER CE CONTEXTE DANS LE SCRIPT
"""
        
        prompt = f"""
Tu es un expert en vente B2B. G√©n√®re un script de cold call ULTRA-PERSONNALIS√â en {nom_langue} avec un ton {ton}.

========================================
TON IDENTIT√â
========================================
- Ton nom : {nom_vendeur}
- Ton entreprise : Gradium

========================================
INFORMATIONS DU PROSPECT
========================================
- Nom : {prospect.nom_complet}
- Entreprise : {prospect.entreprise}
{titre_info}
{taille_info}
{secteur_info}
{statut_info}

{notes_context}

========================================
TRIGGER
========================================
- Type : {trigger.type_trigger}
- Description : {trigger.description}

========================================
INSTRUCTIONS CRITIQUES
========================================

üéØ OBJECTIF : Cr√©er un script qui montre que tu as fait tes recherches sur le prospect et qui g√©n√®re un RDV.

üåê LANGUE : 100% DU SCRIPT DOIT √äTRE EN {nom_langue.upper()} :
- Introduction en {nom_langue}
- Corps du message en {nom_langue}
- Proposition de valeur en {nom_langue}
- Call-to-action en {nom_langue}
- AUCUNE phrase dans une autre langue

‚ö†Ô∏è R√àGLES ABSOLUES :
1. Utilise IMP√âRATIVEMENT les informations des NOTES NOTION ci-dessus
2. Mentionne des √©l√©ments sp√©cifiques trouv√©s dans les notes (Hyper-Recrutement, Pain Points, etc.)
3. Parle en {nom_langue} NATIF (pas de mots fran√ßais si la langue est anglais)
4. Mentionne le pr√©nom du prospect 2-3 fois
5. Sois conversationnel et direct
6. Signe-toi avec ton vrai nom : "{nom_vendeur}"

üìã STRUCTURE (60-90 secondes) :

1. INTRODUCTION (10-15s) :
   "Hi [Pr√©nom], {nom_vendeur} here from Gradium..." (adapter selon la langue)
   ‚Üí Accroche personnalis√©e avec contexte des notes

2. CORPS DU MESSAGE (20-30s) :
   ‚Üí Relie le trigger √† un probl√®me concret mentionn√© dans les notes
   ‚Üí Mentionne 1-2 d√©tails sp√©cifiques des notes

3. PROPOSITION DE VALEUR (15-20s) :
   ‚Üí √âvite "With that kind of..." ou phrases g√©n√©riques
   ‚Üí Donne une proposition CONCR√àTE et CHIFFR√âE si possible
   ‚Üí Exemple : "We help companies like [Entreprise] reduce ramp-up time by 40% through automated signal detection..."
   ‚Üí Explique COMMENT tu r√©sous le probl√®me

4. GESTION D'OBJECTION (10-15s) :
   ‚Üí R√©ponse √† "I'm busy / Not interested / Already have a solution"

5. CALL-TO-ACTION (5-10s) :
   ‚Üí Demande EXPLICITE de RDV STRICTEMENT EN {nom_langue.upper()}
   ‚Üí Exemple EN ANGLAIS : "Can we schedule a 15-minute call this week?"
   ‚Üí Le CTA DOIT √™tre dans la M√äME LANGUE que le reste du script
   ‚Üí INTERDICTION ABSOLUE de m√©langer les langues dans le CTA
   ‚Üí Si le script est en anglais, le CTA DOIT √™tre : "Can we schedule a brief call this week?" ou "Are you available for a quick 15-minute chat?"
   ‚Üí Si le script est en fran√ßais, le CTA DOIT √™tre : "Pouvons-nous convenir d'un appel rapide cette semaine ?"

üö´ INTERDIT :
- Phrases g√©n√©riques comme "With that kind of..."
- M√©langer les langues dans le script
- Parler de soi plus que du prospect
- Oublier de signer avec son nom
- Ne pas utiliser les notes fournies

‚úÖ R√âPONDS UNIQUEMENT AVEC LE SCRIPT COMPLET, sans introduction ni commentaire.
"""
        
        return prompt

    def _parser_script(self, contenu: str, langue: Language) -> Script:
        """
        Parse la r√©ponse de Gemini pour cr√©er un objet Script.
        Simplifi√© pour accepter n'importe quel format de r√©ponse.
        """
        contenu = contenu.strip()
        
        # Si le contenu est vide, retourner un script par d√©faut
        if not contenu:
            return Script(
                introduction="Bonjour, je vous appelle concernant votre entreprise.",
                corps_message="J'ai remarqu√© que vous √™tes en pleine expansion.",
                proposition_valeur="Je peux vous aider √† optimiser vos processus.",
                langue=langue
            )
        
        # D√©couper le contenu en paragraphes
        paragraphes = [p.strip() for p in contenu.split('\n\n') if p.strip()]
        
        # Extraire les 5 parties (ou moins si pas assez de paragraphes)
        introduction = paragraphes[0] if len(paragraphes) > 0 else ""
        corps_message = paragraphes[1] if len(paragraphes) > 1 else ""
        proposition_valeur = paragraphes[2] if len(paragraphes) > 2 else ""
        objection_handling = [paragraphes[3]] if len(paragraphes) > 3 else []
        call_to_action = paragraphes[4] if len(paragraphes) > 4 else "Pouvons-nous convenir d'un rendez-vous ?"
        
        # Nettoyer les titres de section (1., 2., INTRODUCTION, etc.)
        import re
        def nettoyer(texte):
            # Supprimer les num√©ros de section au d√©but
            texte = re.sub(r'^(\d+\.|INTRODUCTION|CORPS|PROPOSITION|OBJECTION|CALL-TO-ACTION)[\s:\-]*', '', texte, flags=re.IGNORECASE)
            return texte.strip()
        
        # Nettoyer les parties
        introduction = nettoyer(introduction)
        corps_message = nettoyer(corps_message)
        proposition_valeur = nettoyer(proposition_valeur)
        objection_handling = [nettoyer(o) for o in objection_handling]
        call_to_action = nettoyer(call_to_action)
        
        return Script(
            introduction=introduction,
            corps_message=corps_message,
            proposition_valeur=proposition_valeur,
            objection_handling=objection_handling,
            call_to_action=call_to_action,
            langue=langue,
            duree_estimee=75  # Dur√©e estim√©e par d√©faut
        )


